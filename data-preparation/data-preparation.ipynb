{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.stats import chi2\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura del dataset iniziale già processato\n",
    "campaign_data = pd.read_csv('../data/campaign-data-processed.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding per \"os_id\"\n",
    "one_hot_encoded = pd.get_dummies(campaign_data['os_id'])\n",
    "campaign_data = pd.concat([campaign_data, one_hot_encoded], axis = 1)\n",
    "campaign_data.drop('os_id', axis = 1, inplace = True)\n",
    "\n",
    "# One-hot encoding per \"browser_id\"\n",
    "one_hot_encoded = pd.get_dummies(campaign_data['browser_id'])\n",
    "campaign_data = pd.concat([campaign_data, one_hot_encoded], axis = 1)\n",
    "campaign_data.drop('browser_id', axis = 1, inplace = True)\n",
    "\n",
    "# One-hot encoding per \"device_type_id\"\n",
    "one_hot_encoded = pd.get_dummies(campaign_data['device_type_id'])\n",
    "campaign_data = pd.concat([campaign_data, one_hot_encoded], axis = 1)\n",
    "campaign_data.drop('device_type_id', axis = 1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assegnazione della label \"clicker\" (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che crea la label \"clicker\" per la riga in input\n",
    "def assignLabel(row):\n",
    "    \n",
    "    if(row['clicks'] >= 1): \n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "# Creazione della label \"clicker\" nel dataset principale\n",
    "campaign_data['clicker'] = campaign_data.apply(lambda row: assignLabel(row), axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcolo dello score (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_class_data = campaign_data[campaign_data['clicker'] == 1].copy()\n",
    "neg_class_data = campaign_data[campaign_data['clicker'] == 0].copy()\n",
    "\n",
    "pos_class_data['score'] = round(pos_class_data['clicks'] / pos_class_data['total_impressions'], 3)\n",
    "neg_class_data['score'] = - round((neg_class_data['total_impressions'] - 1) / neg_class_data['total_impressions'], 3)\n",
    "\n",
    "campaign_data = pd.concat([neg_class_data, pos_class_data])\n",
    "scores = campaign_data['score']\n",
    "\n",
    "campaign_data.drop(['clicks', 'score'], inplace = True, axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suddivisione in training e test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting in training e testing set (classification)\n",
    "campaign_data, testing_set = train_test_split(campaign_data, train_size = 0.75, stratify = campaign_data['clicker'], random_state = 19)\n",
    "testing_set.to_csv('../data/campaign-data-testing.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set regression\n",
    "testing_set['score'] = scores\n",
    "testing_set.drop('clicker', inplace = True, axis = 1)\n",
    "testing_set.to_csv('../data/campaign-data-testing-regression.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminazione dei duplicati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rimozione dei duplicati (maggiore affidabilità e no introduzione di bias)\n",
    "campaign_data = campaign_data.drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "def featureSelection(campaign_data):\n",
    "\n",
    "    # Calcolo della matrice di correlazione tra tutte le variabili del dataframe\n",
    "    corr_matrix = campaign_data.corr()\n",
    "\n",
    "    # Rilevamento delle feature con elevata correlazione positiva o negativa (>= 0.7)\n",
    "    high_corr_feature = corr_matrix[(abs(corr_matrix) >= 0.7) & (abs(corr_matrix) < 1)]\n",
    "    high_corr_feature = high_corr_feature.index[high_corr_feature.any(axis = 1)].tolist()\n",
    "\n",
    "    # Analisi della multicollinearity tra le feature rilevate correlate, utilizzando il VIF\n",
    "    multicollinear_feature = []\n",
    "\n",
    "    # Ciclo che computa il VIF e rimuove una alla votla le variabili problematiche (VIF >= 5)\n",
    "    while(True):\n",
    "\n",
    "        # Computazione del VIF\n",
    "        vif_data = pd.DataFrame(data = high_corr_feature, columns = ['feature'])\n",
    "        vif_data['VIF'] = [ variance_inflation_factor(campaign_data[high_corr_feature].values, i) for i in range(len(high_corr_feature)) ]\n",
    "        vif_data = vif_data.sort_values('VIF', ascending = False)\n",
    "\n",
    "        # VIF con valore più elevato\n",
    "        highest_vif = vif_data['VIF'].iloc[0]\n",
    "\n",
    "        # Se tutte le feature hanno VIF < 5, allora conclude\n",
    "        if(highest_vif < 5):\n",
    "            break\n",
    "        \n",
    "        # Feature con valore del VIF più alto\n",
    "        highest_vif_feature = vif_data['feature'].iloc[0]\n",
    "        multicollinear_feature.append(highest_vif_feature)\n",
    "        high_corr_feature.remove(highest_vif_feature)\n",
    "\n",
    "    # Eliminazione delle feature problematiche\n",
    "    campaign_data_filtered = campaign_data.drop(multicollinear_feature, axis = 1)\n",
    "\n",
    "    return campaign_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection sull'insieme totale delle osservazioni\n",
    "campaign_data = featureSelection(campaign_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rimozione degli outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Funzione che calcola la distanza di Mahalanobis tra ogni riga del dataframe X in input ed il dataframe data (distribuzione) \n",
    "\n",
    "X -> dataframe/osservazioni per le quali calcolare la distanza di Mahalanobis dalla distribuzione\n",
    "data -> dataframe con la distribuzione di partenza\n",
    "cov -> covariance matrix della distribuzione di partenza (if None, viene computata direttamente sulla distribuzione iniziale in input)\n",
    "\n",
    "return -> la distanza di Mahalanobis delle osservazioni di X rispetto alla distribuzione data\n",
    "\"\"\"\n",
    "def computeMahalanobis(X = None, data = None, cov = None):\n",
    "    \n",
    "    # Calcolo della covariance matrix (regularization technique to handle singular covariance matrix) -> regularized Mahalanobis distance\n",
    "    if(not cov):\n",
    "        cov = np.cov(data.values.T) + 0.01 * np.eye(len(data.columns))\n",
    "    \n",
    "    # Mahalanobis Distance formula \n",
    "    return np.round(np.dot((np.dot((X - np.mean(data)), np.linalg.inv(cov))), (X - np.mean(data)).T).diagonal(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (utile per test chi-quadro)\n",
    "clicker = campaign_data['clicker']\n",
    "campaign_data_standardized = campaign_data.drop('clicker', axis = 1).apply(lambda col: StandardScaler().fit_transform(col.values.reshape(-1,1)).flatten())\n",
    "campaign_data_standardized['clicker'] = clicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe positiva -> clicker = 1\n",
    "pos_class_data = campaign_data_standardized[campaign_data_standardized['clicker'] == 1].copy()\n",
    "\n",
    "# Calcolo della distanza di Mahalanobis per ciascuna riga del dataframe rispetto alla distribuzione totale\n",
    "pos_class_data['mahal'] = computeMahalanobis(X = pos_class_data, data = pos_class_data)\n",
    "\n",
    "# Setting del threshold con test chi-quadro (confidenza 0.95)\n",
    "threshold = chi2.ppf((1 - 0.001), df = len(pos_class_data.columns) - 1)\n",
    "\n",
    "# Eliminazione degli outliers in base al threshold stabilito (superato il limite massimo di distanza dalla distribuzione)\n",
    "pos_class_data = pos_class_data[pos_class_data['mahal'] < threshold]\n",
    "\n",
    "# Filtraggio del dataframe non standardizzato\n",
    "pos_class_data = campaign_data.loc[pos_class_data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe negativa -> clicker = 0\n",
    "neg_class_data = campaign_data_standardized[campaign_data_standardized['clicker'] == 0].copy()\n",
    "\n",
    "# Calcolo della distanza di Mahalanobis per ciascuna riga del dataframe rispetto alla distribuzione totale\n",
    "neg_class_data['mahal'] = computeMahalanobis(X = neg_class_data, data = neg_class_data)\n",
    "\n",
    "# Setting del threshold con test chi-quadro (confidenza 0.95)\n",
    "threshold = chi2.ppf((1 - 0.1), df = len(neg_class_data.columns) - 1)\n",
    "\n",
    "# Eliminazione degli outliers in base al threshold stabilito (superato il limite massimo di distanza dalla distribuzione)\n",
    "neg_class_data = neg_class_data[neg_class_data['mahal'] < threshold]\n",
    "\n",
    "# Filtraggio del dataframe non standardizzato\n",
    "neg_class_data = campaign_data.loc[neg_class_data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset globale campaign senza outliers e post feature selection\n",
    "campaign_data = pd.concat([neg_class_data, pos_class_data])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset finale (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrittura del file CSV con training set (classification)\n",
    "campaign_data.to_csv('../data/campaign-data-training.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_data['score'] = scores\n",
    "campaign_data.drop(['clicker', 'total_impressions'], axis = 1, inplace = True)\n",
    "\n",
    "# Scrittura del file CSV con training set (regression)\n",
    "campaign_data.to_csv('../data/campaign-data-training-regression.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
